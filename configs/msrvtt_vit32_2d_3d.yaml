# ============================================
# Full Model: TCN Encoder + Transformer Decoder
# Backbone: CLIP ViT-B/32 2D features (+ 3D + object)
# Dataset: MSR-VTT
# ============================================
project: vctt_vit32
seed: 2271
output_dir: outputs/exp_msrvtt_vit32_2d_3d

# =====================
# Data Configuration
# =====================
num_workers: 6
batch_size: 16
max_frames: 32  # must match feature extraction --max_frames

# Feature dimensions (must match extracted .npy)
feature_dim_2d: 512      # CLIP ViT-B/32 vision features
feature_dim_3d: 768      # Swin3D-T features
feature_dim_obj: 0     # CLIP-ViT pooled object features (or 512 if extractor changed)
use_3d: true
use_obj: false

# =====================
# Dataset Paths
# =====================
splits:
  train: data/msrvtt/splits/msrvtt_train.json
  val:   data/msrvtt/splits/msrvtt_val.json
  test:  data/msrvtt/splits/msrvtt_test_1k.json

# Fallback + per-split feature roots
features_dir: data/msrvtt/features_vit32
features_root:
  train: data/msrvtt/features_vit32/train
  val:   data/msrvtt/features_vit32/val
  test:  data/msrvtt/features_vit32/test

# Multi-reference captions for evaluation (20 refs / video)
msrvtt_raw_captions: data/msrvtt/raw/raw-captions.pkl

# Decoding
beam_width: 5

# =====================
# Tokenizer Configuration
# =====================
tokenizer:
  model_path: outputs/spm/bpe16k.model
  bos_token: <bos>
  eos_token: <eos>
  pad_token: <pad>

# =====================
# Model Configuration
# =====================
model:
  d_model: 512
  dropout: 0.1
  use_3d_stream: true

# TCN Encoder
encoder:
  layers: 6
  kernel_size: 3
  dilations: [1, 2, 4, 8, 16, 32]

# Transformer Decoder
decoder:
  num_layers: 3
  nhead: 8
  dim_feedforward: 2048
  max_len: 40

# =====================
# CLIP Text Encoder (for contrastive video–text alignment)
# =====================
clip:
  text_encoder: openai/clip-vit-base-patch32
  temperature: 0.07

# =====================
# Optimizer & Scheduler
# =====================
optim:
  lr: 1.0e-4
  betas: [0.9, 0.98]
  weight_decay: 0.01
  grad_clip: 1.0

scheduler:
  type: cosine

# =====================
# Loss Configuration
# =====================
loss:
  lambda_obj: 0.0    # encoder–object alignment
  lambda_clip: 0.0   # encoder–CLIP contrastive alignment
  clip_enabled: false

# =====================
# Training Configuration
# =====================
epochs: 25
log_every: 50
